{
  "sha": "92011f61d3aadfb306b8d2a8c872a97d895be21f",
  "url": "https://api.github.com/repos/run-llama/llama_index/commits/92011f61d3aadfb306b8d2a8c872a97d895be21f",
  "files": [
    {
      "sha": "6688e22be048a39850c496f2b862f5393ae9307b",
      "patch": "@@ -0,0 +1,5 @@\n+::: llama_index.graph_stores.tidb\n+    options:\n+      members:\n+        - TiDBGraphStore\n+        - TiDBPropertyGraphStore",
      "status": "added",
      "changes": 5,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fapi_reference%2Fstorage%2Fgraph_stores%2Ftidb.md",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fapi_reference%2Fstorage%2Fgraph_stores%2Ftidb.md",
      "filename": "docs/docs/api_reference/storage/graph_stores/tidb.md",
      "additions": 5,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/docs%2Fdocs%2Fapi_reference%2Fstorage%2Fgraph_stores%2Ftidb.md?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "5404ce4620cc5b83879a69b31999055595dfcecb",
      "patch": "@@ -42,3 +42,12 @@ We support `Amazon Neptune` integrations for both [Neptune Database](https://doc\n See the associated guides below:\n \n - [Amazon Neptune Graph Store](../../examples/index_structs/knowledge_graph/NeptuneDatabaseKGIndexDemo).\n+\n+\n+## `TiDB Graph Store`\n+\n+We support a `TiDBGraphStore` integration, for persisting graphs directly in [TiDB](https://docs.pingcap.com/tidb/stable/overview)!\n+\n+See the associated guides below:\n+\n+- [TiDB Graph Store](../../examples/index_structs/knowledge_graph/TiDBKGIndexDemo.ipynb)",
      "status": "modified",
      "changes": 9,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fcommunity%2Fintegrations%2Fgraph_stores.md",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fcommunity%2Fintegrations%2Fgraph_stores.md",
      "filename": "docs/docs/community/integrations/graph_stores.md",
      "additions": 9,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/docs%2Fdocs%2Fcommunity%2Fintegrations%2Fgraph_stores.md?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "dea3c869285bea983e3c16b88aad9df12b142216",
      "patch": "@@ -0,0 +1,266 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# TiDB Graph Store\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"%pip install llama-index-llms-openai\\n\",\n+    \"%pip install llama-index-graph-stores-tidb\\n\",\n+    \"%pip install llama-index-embeddings-openai\\n\",\n+    \"%pip install llama-index-llms-azure-openai\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# For OpenAI\\n\",\n+    \"\\n\",\n+    \"import os\\n\",\n+    \"\\n\",\n+    \"os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"sk-xxxxxxx\\\"\\n\",\n+    \"\\n\",\n+    \"import logging\\n\",\n+    \"import sys\\n\",\n+    \"from llama_index.llms.openai import OpenAI\\n\",\n+    \"from llama_index.core import Settings\\n\",\n+    \"\\n\",\n+    \"logging.basicConfig(stream=sys.stdout, level=logging.INFO)\\n\",\n+    \"\\n\",\n+    \"# define LLM\\n\",\n+    \"llm = OpenAI(temperature=0, model=\\\"gpt-3.5-turbo\\\")\\n\",\n+    \"Settings.llm = llm\\n\",\n+    \"Settings.chunk_size = 512\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"# For Azure OpenAI\\n\",\n+    \"import os\\n\",\n+    \"import openai\\n\",\n+    \"from llama_index.llms.azure_openai import AzureOpenAI\\n\",\n+    \"from llama_index.embeddings.openai import OpenAIEmbedding\\n\",\n+    \"\\n\",\n+    \"import logging\\n\",\n+    \"import sys\\n\",\n+    \"\\n\",\n+    \"logging.basicConfig(\\n\",\n+    \"    stream=sys.stdout, level=logging.INFO\\n\",\n+    \")  # logging.DEBUG for more verbose output\\n\",\n+    \"logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\\n\",\n+    \"\\n\",\n+    \"openai.api_type = \\\"azure\\\"\\n\",\n+    \"openai.api_base = \\\"https://<foo-bar>.openai.azure.com\\\"\\n\",\n+    \"openai.api_version = \\\"2022-12-01\\\"\\n\",\n+    \"os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"<your-openai-key>\\\"\\n\",\n+    \"openai.api_key = os.getenv(\\\"OPENAI_API_KEY\\\")\\n\",\n+    \"\\n\",\n+    \"llm = AzureOpenAI(\\n\",\n+    \"    deployment_name=\\\"<foo-bar-deployment>\\\",\\n\",\n+    \"    temperature=0,\\n\",\n+    \"    openai_api_version=openai.api_version,\\n\",\n+    \"    model_kwargs={\\n\",\n+    \"        \\\"api_key\\\": openai.api_key,\\n\",\n+    \"        \\\"api_base\\\": openai.api_base,\\n\",\n+    \"        \\\"api_type\\\": openai.api_type,\\n\",\n+    \"        \\\"api_version\\\": openai.api_version,\\n\",\n+    \"    },\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# You need to deploy your own embedding model as well as your own chat completion model\\n\",\n+    \"embedding_llm = OpenAIEmbedding(\\n\",\n+    \"    model=\\\"text-embedding-ada-002\\\",\\n\",\n+    \"    deployment_name=\\\"<foo-bar-deployment>\\\",\\n\",\n+    \"    api_key=openai.api_key,\\n\",\n+    \"    api_base=openai.api_base,\\n\",\n+    \"    api_type=openai.api_type,\\n\",\n+    \"    api_version=openai.api_version,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"Settings.llm = llm\\n\",\n+    \"Settings.embed_model = embedding_llm\\n\",\n+    \"Settings.chunk_size = 512\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Using Knowledge Graph with TiDB\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Prepare a TiDB cluster\\n\",\n+    \"\\n\",\n+    \"- [TiDB Cloud](https://tidb.cloud/) [Recommended], a fully managed TiDB service that frees you from the complexity of database operations.\\n\",\n+    \"- [TiUP](https://docs.pingcap.com/tidb/stable/tiup-overview), use `tiup playground`` to create a local TiDB cluster for testing.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"#### Get TiDB connection string\\n\",\n+    \"\\n\",\n+    \"For example: `mysql+pymysql://user:password@host:4000/dbname`, in TiDBGraphStore we use pymysql as the db driver, so the connection string should be `mysql+pymysql://...`.\\n\",\n+    \"\\n\",\n+    \"If you are using a TiDB Cloud serverless cluster with public endpoint, it requires TLS connection, so the connection string should be like `mysql+pymysql://user:password@host:4000/dbname?ssl_verify_cert=true&ssl_verify_identity=true`.\\n\",\n+    \"\\n\",\n+    \"Replace `user`, `password`, `host`, `dbname` with your own values.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Initialize TiDBGraphStore\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from llama_index.graph_stores.tidb import TiDBGraphStore\\n\",\n+    \"\\n\",\n+    \"graph_store = TiDBGraphStore(\\n\",\n+    \"    db_connection_string=\\\"mysql+pymysql://user:password@host:4000/dbname\\\"\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"### Instantiate TiDB KG Indexes\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from llama_index.core import (\\n\",\n+    \"    KnowledgeGraphIndex,\\n\",\n+    \"    SimpleDirectoryReader,\\n\",\n+    \"    StorageContext,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"documents = SimpleDirectoryReader(\\n\",\n+    \"    \\\"../../../examples/data/paul_graham/\\\"\\n\",\n+    \").load_data()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"storage_context = StorageContext.from_defaults(graph_store=graph_store)\\n\",\n+    \"\\n\",\n+    \"# NOTE: can take a while!\\n\",\n+    \"index = KnowledgeGraphIndex.from_documents(\\n\",\n+    \"    documents=documents,\\n\",\n+    \"    storage_context=storage_context,\\n\",\n+    \"    max_triplets_per_chunk=2,\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"#### Querying the Knowledge Graph\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\\n\",\n+      \"WARNING:llama_index.core.indices.knowledge_graph.retrievers:Index was not constructed with embeddings, skipping embedding usage...\\n\",\n+      \"INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \\\"HTTP/1.1 200 OK\\\"\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"query_engine = index.as_query_engine(\\n\",\n+    \"    include_text=False, response_mode=\\\"tree_summarize\\\"\\n\",\n+    \")\\n\",\n+    \"response = query_engine.query(\\n\",\n+    \"    \\\"Tell me more about Interleaf\\\",\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"text/markdown\": [\n+       \"<b>Interleaf was a software company that developed a scripting language and was known for its software products. It was inspired by Emacs and faced challenges due to Moore's law. Over time, Interleaf's prominence declined.</b>\"\n+      ],\n+      \"text/plain\": [\n+       \"<IPython.core.display.Markdown object>\"\n+      ]\n+     },\n+     \"metadata\": {},\n+     \"output_type\": \"display_data\"\n+    }\n+   ],\n+   \"source\": [\n+    \"from IPython.display import Markdown, display\\n\",\n+    \"\\n\",\n+    \"display(Markdown(f\\\"<b>{response}</b>\\\"))\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \".venv\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}",
      "status": "added",
      "changes": 266,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fexamples%2Findex_structs%2Fknowledge_graph%2FTiDBKGIndexDemo.ipynb",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fexamples%2Findex_structs%2Fknowledge_graph%2FTiDBKGIndexDemo.ipynb",
      "filename": "docs/docs/examples/index_structs/knowledge_graph/TiDBKGIndexDemo.ipynb",
      "additions": 266,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/docs%2Fdocs%2Fexamples%2Findex_structs%2Fknowledge_graph%2FTiDBKGIndexDemo.ipynb?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "0f71c1578a32dc90f6f523211cfacbecfa04e707",
      "patch": "@@ -0,0 +1,317 @@\n+{\n+ \"cells\": [\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"# TiDB Property Graph Index\\n\",\n+    \"\\n\",\n+    \"<a href=\\\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_tidb.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\\n\",\n+    \"\\n\",\n+    \"\\n\",\n+    \"TiDB is a distributed SQL database, it is MySQL compatible and features horizontal scalability, strong consistency, and high availability. Currently it only supports Vector Search in [TiDB Cloud Serverless](https://tidb.cloud/ai).\\n\",\n+    \"\\n\",\n+    \"In this nodebook, we will cover how to connect to a TiDB Serverless cluster and create a property graph index.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"%pip install llama-index llama-index-graph-stores-tidb\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Prepare TiDB Serverless Cluster\\n\",\n+    \"\\n\",\n+    \"Sign up for [TiDB Cloud](https://tidb.cloud/) and create a TiDB Serverless cluster with Vector Search enabled.\\n\",\n+    \"\\n\",\n+    \"Get the db connection string from the Cluster Details page, for example:\\n\",\n+    \"\\n\",\n+    \"```\\n\",\n+    \"mysql+pymysql://user:password@host:4000/dbname?ssl_verify_cert=true&ssl_verify_identity=true\\n\",\n+    \"```\\n\",\n+    \"\\n\",\n+    \"TiDB Serverless requires TSL connection when using public endpoint.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Env Setup\\n\",\n+    \"\\n\",\n+    \"We need just a few environment setups to get started.\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import os\\n\",\n+    \"\\n\",\n+    \"os.environ[\\\"OPENAI_API_KEY\\\"] = \\\"sk-proj-...\\\"\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"!mkdir -p 'data/paul_graham/'\\n\",\n+    \"!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"import nest_asyncio\\n\",\n+    \"\\n\",\n+    \"nest_asyncio.apply()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from llama_index.core import SimpleDirectoryReader\\n\",\n+    \"\\n\",\n+    \"documents = SimpleDirectoryReader(\\\"./data/paul_graham/\\\").load_data()\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Index Construction\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"data\": {\n+      \"application/vnd.jupyter.widget-view+json\": {\n+       \"model_id\": \"9a7bffce7add4c59ae0b7daa6945cfae\",\n+       \"version_major\": 2,\n+       \"version_minor\": 0\n+      },\n+      \"text/plain\": [\n+       \"Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]\"\n+      ]\n+     },\n+     \"metadata\": {},\n+     \"output_type\": \"display_data\"\n+    },\n+    {\n+     \"name\": \"stderr\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Extracting paths from text with schema: 100%|██████████| 22/22 [00:44<00:00,  2.02s/it]\\n\",\n+      \"Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\\n\",\n+      \"Generating embeddings: 100%|██████████| 3/3 [00:01<00:00,  1.51it/s]\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"from llama_index.core import PropertyGraphIndex\\n\",\n+    \"from llama_index.embeddings.openai import OpenAIEmbedding\\n\",\n+    \"from llama_index.llms.openai import OpenAI\\n\",\n+    \"from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\\n\",\n+    \"from llama_index.graph_stores.tidb import TiDBPropertyGraphStore\\n\",\n+    \"\\n\",\n+    \"graph_store = TiDBPropertyGraphStore(\\n\",\n+    \"    db_connection_string=\\\"mysql+pymysql://user:password@host:4000/dbname?ssl_verify_cert=true&ssl_verify_identity=true\\\",\\n\",\n+    \"    drop_existing_table=True,\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"# Note: it can take a while to index the documents, especially if you have a large number of documents.\\n\",\n+    \"# Especially if you are connecting TiDB Serverless to a public endpoint, it depends on the distance between your server location and the TiDB serverless location.\\n\",\n+    \"index = PropertyGraphIndex.from_documents(\\n\",\n+    \"    documents,\\n\",\n+    \"    embed_model=OpenAIEmbedding(model_name=\\\"text-embedding-3-small\\\"),\\n\",\n+    \"    kg_extractors=[\\n\",\n+    \"        SchemaLLMPathExtractor(\\n\",\n+    \"            llm=OpenAI(model=\\\"gpt-3.5-turbo\\\", temperature=0.0)\\n\",\n+    \"        )\\n\",\n+    \"    ],\\n\",\n+    \"    property_graph_store=graph_store,\\n\",\n+    \"    show_progress=True,\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Querying and Retrieval\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Interleaf -> USED_FOR -> software for creating documents\\n\",\n+      \"Interleaf -> HAS -> scripting language\\n\",\n+      \"Interleaf -> HAS -> Lisp\\n\",\n+      \"Viaweb -> USED_FOR -> site builders\\n\",\n+      \"Viaweb -> USED_FOR -> ecommerce software\\n\",\n+      \"Viaweb -> USED_FOR -> retail\\n\",\n+      \"Viaweb -> USED_FOR -> business\\n\",\n+      \"Viaweb -> IS_A -> application service provider\\n\",\n+      \"Viaweb -> IS_A -> software as a service\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"retriever = index.as_retriever(\\n\",\n+    \"    include_text=False,  # include source text in returned nodes, default True\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"nodes = retriever.retrieve(\\\"What happened at Interleaf and Viaweb?\\\")\\n\",\n+    \"\\n\",\n+    \"for node in nodes:\\n\",\n+    \"    print(node.text)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Interleaf added a scripting language inspired by Emacs, which was a dialect of Lisp. The individual who worked at Interleaf found the Lisp implementation challenging due to their lack of knowledge in C. They also learned various lessons about technology companies and office dynamics during their time at Interleaf. On the other hand, Viaweb was used for site builders, ecommerce software, retail, and business purposes. The work on Viaweb and Y Combinator initially seemed unimpressive and lacked prestige, but the individual found success by working on less prestigious projects.\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"query_engine = index.as_query_engine(include_text=True)\\n\",\n+    \"\\n\",\n+    \"response = query_engine.query(\\\"What happened at Interleaf and Viaweb?\\\")\\n\",\n+    \"\\n\",\n+    \"print(str(response))\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"## Loading from an existing Graph\\n\",\n+    \"\\n\",\n+    \"If you have an existing graph (either created with LlamaIndex or otherwise), we can connect to and use it!\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from llama_index.core import PropertyGraphIndex\\n\",\n+    \"from llama_index.embeddings.openai import OpenAIEmbedding\\n\",\n+    \"from llama_index.llms.openai import OpenAI\\n\",\n+    \"from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\\n\",\n+    \"from llama_index.graph_stores.tidb import TiDBPropertyGraphStore\\n\",\n+    \"\\n\",\n+    \"graph_store = TiDBPropertyGraphStore(\\n\",\n+    \"    db_connection_string=\\\"mysql+pymysql://user:password@host:4000/dbname?ssl_verify_cert=true&ssl_verify_identity=true\\\",\\n\",\n+    \")\\n\",\n+    \"\\n\",\n+    \"index = PropertyGraphIndex.from_existing(\\n\",\n+    \"    property_graph_store=graph_store,\\n\",\n+    \"    llm=OpenAI(model=\\\"gpt-3.5-turbo\\\", temperature=0.3),\\n\",\n+    \"    embed_model=OpenAIEmbedding(model_name=\\\"text-embedding-3-small\\\"),\\n\",\n+    \")\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"From here, we can still insert more documents!\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [],\n+   \"source\": [\n+    \"from llama_index.core import Document\\n\",\n+    \"\\n\",\n+    \"document = Document(text=\\\"LlamaIndex is great!\\\")\\n\",\n+    \"\\n\",\n+    \"index.insert(document)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"code\",\n+   \"execution_count\": null,\n+   \"metadata\": {},\n+   \"outputs\": [\n+    {\n+     \"name\": \"stdout\",\n+     \"output_type\": \"stream\",\n+     \"text\": [\n+      \"Llamaindex -> Is -> Great\\n\"\n+     ]\n+    }\n+   ],\n+   \"source\": [\n+    \"nodes = index.as_retriever(include_text=False).retrieve(\\\"LlamaIndex\\\")\\n\",\n+    \"\\n\",\n+    \"print(nodes[0].text)\"\n+   ]\n+  },\n+  {\n+   \"cell_type\": \"markdown\",\n+   \"metadata\": {},\n+   \"source\": [\n+    \"For full details on construction, retrieval, querying of a property graph, see the [full docs page](../../module_guides/indexing/lpg_index_guide.md).\"\n+   ]\n+  }\n+ ],\n+ \"metadata\": {\n+  \"kernelspec\": {\n+   \"display_name\": \".venv\",\n+   \"language\": \"python\",\n+   \"name\": \"python3\"\n+  },\n+  \"language_info\": {\n+   \"codemirror_mode\": {\n+    \"name\": \"ipython\",\n+    \"version\": 3\n+   },\n+   \"file_extension\": \".py\",\n+   \"mimetype\": \"text/x-python\",\n+   \"name\": \"python\",\n+   \"nbconvert_exporter\": \"python\",\n+   \"pygments_lexer\": \"ipython3\"\n+  }\n+ },\n+ \"nbformat\": 4,\n+ \"nbformat_minor\": 2\n+}",
      "status": "added",
      "changes": 317,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fexamples%2Fproperty_graph%2Fproperty_graph_tidb.ipynb",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fexamples%2Fproperty_graph%2Fproperty_graph_tidb.ipynb",
      "filename": "docs/docs/examples/property_graph/property_graph_tidb.ipynb",
      "additions": 317,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/docs%2Fdocs%2Fexamples%2Fproperty_graph%2Fproperty_graph_tidb.ipynb?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "f2db91e646b57f7c4acef843bbe0edaef25c3897",
      "patch": "@@ -358,6 +358,7 @@ Currently, supported graph stores for property graphs include:\n | SimplePropertyGraphStore | ✅         | ❌                        | ❌     | Disk                  |\n | Neo4jPropertyGraphStore  | ❌         | ✅                        | ❌     | Server                |\n | NebulaPropertyGraphStore | ❌         | ❌                        | ❌     | Server                |\n+| TiDBPropertyGraphStore   | ❌         | ✅                        | ❌     | Server                |\n \n ### Saving to/from disk\n ",
      "status": "modified",
      "changes": 1,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fmodule_guides%2Findexing%2Flpg_index_guide.md",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/docs%2Fdocs%2Fmodule_guides%2Findexing%2Flpg_index_guide.md",
      "filename": "docs/docs/module_guides/indexing/lpg_index_guide.md",
      "additions": 1,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/docs%2Fdocs%2Fmodule_guides%2Findexing%2Flpg_index_guide.md?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "990c18de229088f55c6c514fd0f2d49981d1b0e7",
      "patch": "@@ -0,0 +1,153 @@\n+llama_index/_static\n+.DS_Store\n+# Byte-compiled / optimized / DLL files\n+__pycache__/\n+*.py[cod]\n+*$py.class\n+\n+# C extensions\n+*.so\n+\n+# Distribution / packaging\n+.Python\n+bin/\n+build/\n+develop-eggs/\n+dist/\n+downloads/\n+eggs/\n+.eggs/\n+etc/\n+include/\n+lib/\n+lib64/\n+parts/\n+sdist/\n+share/\n+var/\n+wheels/\n+pip-wheel-metadata/\n+share/python-wheels/\n+*.egg-info/\n+.installed.cfg\n+*.egg\n+MANIFEST\n+\n+# PyInstaller\n+#  Usually these files are written by a python script from a template\n+#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n+*.manifest\n+*.spec\n+\n+# Installer logs\n+pip-log.txt\n+pip-delete-this-directory.txt\n+\n+# Unit test / coverage reports\n+htmlcov/\n+.tox/\n+.nox/\n+.coverage\n+.coverage.*\n+.cache\n+nosetests.xml\n+coverage.xml\n+*.cover\n+*.py,cover\n+.hypothesis/\n+.pytest_cache/\n+.ruff_cache\n+\n+# Translations\n+*.mo\n+*.pot\n+\n+# Django stuff:\n+*.log\n+local_settings.py\n+db.sqlite3\n+db.sqlite3-journal\n+\n+# Flask stuff:\n+instance/\n+.webassets-cache\n+\n+# Scrapy stuff:\n+.scrapy\n+\n+# Sphinx documentation\n+docs/_build/\n+\n+# PyBuilder\n+target/\n+\n+# Jupyter Notebook\n+.ipynb_checkpoints\n+notebooks/\n+\n+# IPython\n+profile_default/\n+ipython_config.py\n+\n+# pyenv\n+.python-version\n+\n+# pipenv\n+#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n+#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n+#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n+#   install all needed dependencies.\n+#Pipfile.lock\n+\n+# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n+__pypackages__/\n+\n+# Celery stuff\n+celerybeat-schedule\n+celerybeat.pid\n+\n+# SageMath parsed files\n+*.sage.py\n+\n+# Environments\n+.env\n+.venv\n+env/\n+venv/\n+ENV/\n+env.bak/\n+venv.bak/\n+pyvenv.cfg\n+\n+# Spyder project settings\n+.spyderproject\n+.spyproject\n+\n+# Rope project settings\n+.ropeproject\n+\n+# mkdocs documentation\n+/site\n+\n+# mypy\n+.mypy_cache/\n+.dmypy.json\n+dmypy.json\n+\n+# Pyre type checker\n+.pyre/\n+\n+# Jetbrains\n+.idea\n+modules/\n+*.swp\n+\n+# VsCode\n+.vscode\n+\n+# pipenv\n+Pipfile\n+Pipfile.lock\n+\n+# pyright\n+pyrightconfig.json",
      "status": "added",
      "changes": 153,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2F.gitignore",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2F.gitignore",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/.gitignore",
      "additions": 153,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2F.gitignore?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "0896ca890d8bffd60a44fa824f8d57fecd73ee53",
      "patch": "@@ -0,0 +1,3 @@\n+poetry_requirements(\n+    name=\"poetry\",\n+)",
      "status": "added",
      "changes": 3,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FBUILD",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FBUILD",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/BUILD",
      "additions": 3,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FBUILD?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "b9eab05aa370629a4a3de75df3ff64cd53887b68",
      "patch": "@@ -0,0 +1,17 @@\n+GIT_ROOT ?= $(shell git rev-parse --show-toplevel)\n+\n+help:\t## Show all Makefile targets.\n+\t@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[33m%-30s\\033[0m %s\\n\", $$1, $$2}'\n+\n+format:\t## Run code autoformatters (black).\n+\tpre-commit install\n+\tgit ls-files | xargs pre-commit run black --files\n+\n+lint:\t## Run linters: pre-commit (black, ruff, codespell) and mypy\n+\tpre-commit install && git ls-files | xargs pre-commit run --show-diff-on-failure --files\n+\n+test:\t## Run tests via pytest.\n+\tpytest tests\n+\n+watch-docs:\t## Build and watch documentation.\n+\tsphinx-autobuild docs/ docs/_build/html --open-browser --watch $(GIT_ROOT)/llama_index/",
      "status": "added",
      "changes": 17,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FMakefile",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FMakefile",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/Makefile",
      "additions": 17,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FMakefile?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "15ecc1c7f5385f494a233262b6adb25166cf788a",
      "patch": "@@ -0,0 +1,94 @@\n+# LlamaIndex Graph Stores Integration: TiDB\n+\n+TiDB is a distributed SQL database, it is MySQL compatible and features horizontal scalability, strong consistency, and high availability. Currently it also supports Vector Search in [TiDB Cloud Serverless](https://tidb.cloud/ai).\n+\n+In this project, we integrate TiDB as the graph store to store the LlamaIndex graph data, and use TiDB's SQL interface to query the graph data. so that people can use TiDB to interact with LlamaIndex graph index.\n+\n+- Property Graph Store: `TiDBPropertyGraphStore`\n+- Knowledge Graph Store: `TiDBGraphStore`\n+\n+## Installation\n+\n+```shell\n+pip install llama-index llama-index-graph-stores-tidb\n+```\n+\n+## Usage\n+\n+### Property Graph Store\n+\n+NOTE: `TiDBPropertyGraphStore` requires the Vector Search feature in TiDB, but now it is only available in [TiDB Cloud Serverless](https://tidb.cloud/ai).\n+\n+Please checkout this [tutorial](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/property_graph/property_graph_tidb.ipynb) to learn how to use `TiDBPropertyGraphStore` with LlamaIndex.\n+\n+Simple example to use `TiDBPropertyGraphStore`:\n+\n+```python\n+from llama_index.core import PropertyGraphIndex, SimpleDirectoryReader\n+from llama_index.embeddings.openai import OpenAIEmbedding\n+from llama_index.llms.openai import OpenAI\n+from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n+from llama_index.graph_stores.tidb import TiDBPropertyGraphStore\n+\n+documents = SimpleDirectoryReader(\n+    \"../../../examples/data/paul_graham/\"\n+).load_data()\n+\n+graph_store = TiDBPropertyGraphStore(\n+    db_connection_string=\"mysql+pymysql://user:password@host:4000/dbname?ssl_verify_cert=true&ssl_verify_identity=true\",\n+)\n+\n+index = PropertyGraphIndex.from_documents(\n+    documents,\n+    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n+    kg_extractors=[\n+        SchemaLLMPathExtractor(\n+            llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n+        )\n+    ],\n+    property_graph_store=graph_store,\n+    show_progress=True,\n+)\n+\n+query_engine = index.as_query_engine(include_text=True)\n+response = query_engine.query(\"What happened at Interleaf and Viaweb?\")\n+print(response)\n+```\n+\n+### Knowledge Graph Store\n+\n+Checkout this [tutorial](https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/index_structs/knowledge_graph/TiDBKGIndexDemo.ipynb) to learn how to use `TiDBGraphStore` with LlamaIndex.\n+\n+For `TiDBGraphStore`, you can use either Self-Hosted TiDB or TiDB Cloud Serverless(Recommended).\n+\n+Simple example to use `TiDBGraphStore`:\n+\n+```python\n+from llama_index.graph_stores.tidb import TiDBGraphStore\n+from llama_index.core import (\n+    KnowledgeGraphIndex,\n+    SimpleDirectoryReader,\n+    StorageContext,\n+)\n+\n+documents = SimpleDirectoryReader(\n+    \"../../../examples/data/paul_graham/\"\n+).load_data()\n+\n+graph_store = TiDBGraphStore(\n+    db_connection_string=\"mysql+pymysql://user:password@host:4000/dbname\"\n+)\n+storage_context = StorageContext.from_defaults(graph_store=graph_store)\n+index = KnowledgeGraphIndex.from_documents(\n+    documents=documents,\n+    storage_context=storage_context,\n+    max_triplets_per_chunk=2,\n+)\n+query_engine = index.as_query_engine(\n+    include_text=False, response_mode=\"tree_summarize\"\n+)\n+response = query_engine.query(\n+    \"Tell me more about Interleaf\",\n+)\n+print(response)\n+```",
      "status": "added",
      "changes": 94,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FREADME.md",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FREADME.md",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/README.md",
      "additions": 94,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2FREADME.md?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "db46e8d6c978c67e301dd6c47bee08c1b3fd141c",
      "patch": "@@ -0,0 +1 @@\n+python_sources()",
      "status": "added",
      "changes": 1,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2FBUILD",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2FBUILD",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/BUILD",
      "additions": 1,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2FBUILD?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "8009d83ad1da8c93681871b50e5a25f9dbdeeb24",
      "patch": "@@ -0,0 +1,4 @@\n+from llama_index.graph_stores.tidb.graph import TiDBGraphStore\n+from llama_index.graph_stores.tidb.property_graph import TiDBPropertyGraphStore\n+\n+__all__ = [\"TiDBGraphStore\", \"TiDBPropertyGraphStore\"]",
      "status": "added",
      "changes": 4,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2F__init__.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2F__init__.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/__init__.py",
      "additions": 4,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2F__init__.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "71cb81810dc775085fe777cb473f9da4272ebd2a",
      "patch": "@@ -0,0 +1,223 @@\n+\"\"\"TiDB graph store index.\"\"\"\n+from typing import Tuple, Any, List, Optional, Dict\n+from collections import defaultdict\n+from sqlalchemy import (\n+    create_engine,\n+    Column,\n+    Integer,\n+    String,\n+    DateTime,\n+    ForeignKey,\n+    Text,\n+    sql,\n+    delete,\n+)\n+from sqlalchemy.orm import (\n+    Session,\n+    declarative_base,\n+    relationship,\n+    joinedload,\n+)\n+\n+from llama_index.core.graph_stores.types import GraphStore\n+from llama_index.graph_stores.tidb.utils import check_db_availability, get_or_create\n+\n+\n+rel_depth_query = \"\"\"\n+WITH RECURSIVE PATH AS\n+  (SELECT 1 AS depth,\n+          r.subject_id,\n+          r.object_id,\n+          r.description\n+   FROM {relation_table} r\n+   WHERE r.subject_id IN\n+       (SELECT id\n+        FROM {entity_table}\n+        WHERE name IN :subjs )\n+   UNION ALL SELECT p.depth + 1,\n+                    r.subject_id,\n+                    r.object_id,\n+                    r.description\n+   FROM PATH p\n+   JOIN {relation_table} r ON p.object_id = r.subject_id\n+   WHERE p.depth < :depth )\n+SELECT p.depth,\n+       e1.name AS subject,\n+       p.description,\n+       e2.name AS object\n+FROM PATH p\n+JOIN {entity_table} e1 ON p.subject_id = e1.id\n+JOIN {entity_table} e2 ON p.object_id = e2.id\n+ORDER BY p.depth\n+LIMIT :limit;\n+\"\"\"\n+\n+\n+class TiDBGraphStore(GraphStore):\n+    def __init__(\n+        self,\n+        db_connection_string: str,\n+        entity_table_name: str = \"entities\",\n+        relation_table_name: str = \"relations\",\n+    ) -> None:\n+        # TiDB Serverless clusters have a limitation: if there are no active connections for 5 minutes,\n+        # they will shut down, which closes all connections, so we need to recycle the connections\n+        self._engine = create_engine(db_connection_string, pool_recycle=300)\n+        check_db_availability(self._engine)\n+\n+        self._entity_table_name = entity_table_name\n+        self._relation_table_name = relation_table_name\n+        self._entity_model, self._rel_model = self.init_schema()\n+\n+    def init_schema(self) -> Tuple[Any, Any]:\n+        \"\"\"Initialize schema.\"\"\"\n+        Base = declarative_base()\n+\n+        class EntityModel(Base):\n+            __tablename__ = self._entity_table_name\n+\n+            id = Column(Integer, primary_key=True)\n+            name = Column(String(512), nullable=False)\n+            created_at = Column(DateTime, nullable=False, server_default=sql.func.now())\n+            updated_at = Column(\n+                DateTime,\n+                nullable=False,\n+                server_default=sql.func.now(),\n+                onupdate=sql.func.now(),\n+            )\n+\n+        class RelationshipModel(Base):\n+            __tablename__ = self._relation_table_name\n+\n+            id = Column(Integer, primary_key=True)\n+            description = Column(Text, nullable=False)\n+            subject_id = Column(Integer, ForeignKey(f\"{self._entity_table_name}.id\"))\n+            object_id = Column(Integer, ForeignKey(f\"{self._entity_table_name}.id\"))\n+            created_at = Column(DateTime, nullable=False, server_default=sql.func.now())\n+            updated_at = Column(\n+                DateTime,\n+                nullable=False,\n+                server_default=sql.func.now(),\n+                onupdate=sql.func.now(),\n+            )\n+\n+            subject = relationship(\"EntityModel\", foreign_keys=[subject_id])\n+            object = relationship(\"EntityModel\", foreign_keys=[object_id])\n+\n+        Base.metadata.create_all(self._engine)\n+        return EntityModel, RelationshipModel\n+\n+    @property\n+    def get_client(self) -> Any:\n+        \"\"\"Get client.\"\"\"\n+        return self._engine\n+\n+    def upsert_triplet(self, subj: str, rel: str, obj: str) -> None:\n+        \"\"\"Add triplet.\"\"\"\n+        with Session(self._engine) as session:\n+            subj_instance, _ = get_or_create(session, self._entity_model, name=subj)\n+            obj_instance, _ = get_or_create(session, self._entity_model, name=obj)\n+            get_or_create(\n+                session,\n+                self._rel_model,\n+                description=rel,\n+                subject=subj_instance,\n+                object=obj_instance,\n+            )\n+\n+    def get(self, subj: str) -> List[List[str]]:\n+        \"\"\"Get triplets.\"\"\"\n+        with Session(self._engine) as session:\n+            rels = (\n+                session.query(self._rel_model)\n+                .options(\n+                    joinedload(self._rel_model.subject),\n+                    joinedload(self._rel_model.object),\n+                )\n+                .filter(self._rel_model.subject.has(name=subj))\n+                .all()\n+            )\n+            return [[rel.description, rel.object.name] for rel in rels]\n+\n+    def get_rel_map(\n+        self, subjs: Optional[List[str]] = None, depth: int = 2, limit: int = 30\n+    ) -> Dict[str, List[List[str]]]:\n+        \"\"\"Get depth-aware rel map.\"\"\"\n+        rel_map: Dict[str, List[List[str]]] = defaultdict(list)\n+        with Session(self._engine) as session:\n+            # `raw_rels`` is a list of tuples (depth, subject, description, object), ordered by depth\n+            # Example:\n+            # +-------+------------------+------------------+------------------+\n+            # | depth | subject          | description      | object           |\n+            # +-------+------------------+------------------+------------------+\n+            # |     1 | Software         | Mention in       | Footnotes        |\n+            # |     1 | Viaweb           | Started by       | Paul graham      |\n+            # |     2 | Paul graham      | Invited to       | Lisp conference  |\n+            # |     2 | Paul graham      | Coded            | Bel              |\n+            # +-------+------------------+------------------+------------------+\n+            raw_rels = session.execute(\n+                sql.text(\n+                    rel_depth_query.format(\n+                        relation_table=self._relation_table_name,\n+                        entity_table=self._entity_table_name,\n+                    )\n+                ),\n+                {\n+                    \"subjs\": subjs,\n+                    \"depth\": depth,\n+                    \"limit\": limit,\n+                },\n+            ).fetchall()\n+            # `obj_reverse_map` is a dict of sets, where the key is a tuple (object, depth)\n+            # and the value is a set of subjects that have the object at the previous depth\n+            obj_reverse_map = defaultdict(set)\n+            for depth, subj, rel, obj in raw_rels:\n+                if depth == 1:\n+                    rel_map[subj].append([subj, rel, obj])\n+                    obj_reverse_map[(obj, depth)].update([subj])\n+                else:\n+                    for _subj in obj_reverse_map[(subj, depth - 1)]:\n+                        rel_map[_subj].append([subj, rel, obj])\n+                        obj_reverse_map[(obj, depth)].update([_subj])\n+            return dict(rel_map)\n+\n+    def delete(self, subj: str, rel: str, obj: str) -> None:\n+        \"\"\"Delete triplet.\"\"\"\n+        with Session(self._engine) as session:\n+            stmt = delete(self._rel_model).where(\n+                self._rel_model.subject.has(name=subj),\n+                self._rel_model.description == rel,\n+                self._rel_model.object.has(name=obj),\n+            )\n+            result = session.execute(stmt)\n+            session.commit()\n+            # no rows affected, do not need to delete entities\n+            if result.rowcount == 0:\n+                return\n+\n+            def delete_entity(entity_name: str):\n+                stmt = delete(self._entity_model).where(\n+                    self._entity_model.name == entity_name\n+                )\n+                session.execute(stmt)\n+                session.commit()\n+\n+            def entity_was_referenced(entity_name: str):\n+                return (\n+                    session.query(self._rel_model)\n+                    .filter(\n+                        self._rel_model.subject.has(name=entity_name)\n+                        | self._rel_model.object.has(name=entity_name)\n+                    )\n+                    .one_or_none()\n+                )\n+\n+            if not entity_was_referenced(subj):\n+                delete_entity(subj)\n+            if not entity_was_referenced(obj):\n+                delete_entity(obj)\n+\n+    def query(self, query: str, param_map: Optional[Dict[str, Any]] = {}) -> Any:\n+        \"\"\"Query the graph store with statement and parameters.\"\"\"\n+        with Session(self._engine) as session:\n+            return session.execute(query, param_map).fetchall()",
      "status": "added",
      "changes": 223,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Fgraph.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Fgraph.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/graph.py",
      "additions": 223,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Fgraph.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "414412e3118f9f7447702ecfc436a1bf2fbb4e84",
      "patch": "@@ -0,0 +1,451 @@\n+\"\"\"TiDB property graph store index.\"\"\"\n+import json\n+from typing import Tuple, Optional, List, Dict, Any\n+from sqlalchemy import (\n+    create_engine,\n+    Column,\n+    Integer,\n+    String,\n+    DateTime,\n+    JSON,\n+    TEXT,\n+    ForeignKey,\n+    sql,\n+    delete,\n+)\n+from sqlalchemy.orm import (\n+    Session,\n+    declarative_base,\n+    relationship,\n+    joinedload,\n+)\n+\n+from tidb_vector.sqlalchemy import VectorType\n+from llama_index.core.graph_stores.types import (\n+    PropertyGraphStore,\n+    LabelledNode,\n+    EntityNode,\n+    ChunkNode,\n+    Relation,\n+    Triplet,\n+    VectorStoreQuery,\n+)\n+from llama_index.graph_stores.tidb.utils import (\n+    check_db_availability,\n+    remove_empty_values,\n+    get_or_create,\n+)\n+\n+\n+rel_depth_query = \"\"\"\n+WITH RECURSIVE PATH AS\n+  (SELECT 1 AS depth,\n+          r.source_id,\n+          r.target_id,\n+          r.label,\n+          r.properties\n+   FROM {relation_table} r\n+   WHERE r.source_id IN :ids\n+   UNION ALL SELECT p.depth + 1,\n+                    r.source_id,\n+                    r.target_id,\n+                    r.label,\n+                    r.properties\n+   FROM PATH p\n+   JOIN {relation_table} r ON p.target_id = r.source_id\n+   WHERE p.depth < :depth )\n+SELECT e1.id AS e1_id,\n+       e1.name AS e1_name,\n+       e1.label AS e1_label,\n+       e1.properties AS e1_properties,\n+       p.label AS rel_label,\n+       p.properties AS rel_properties,\n+       e2.id AS e2_id,\n+       e2.name AS e2_name,\n+       e2.label AS e2_label,\n+       e2.properties AS e2_properties\n+FROM PATH p\n+JOIN {node_table} e1 ON p.source_id = e1.id\n+JOIN {node_table} e2 ON p.target_id = e2.id\n+ORDER BY p.depth\n+LIMIT :limit;\n+\"\"\"\n+\n+\n+class TiDBPropertyGraphStore(PropertyGraphStore):\n+    # TiDB does not support graph cypher queries\n+    supports_structured_queries: bool = False\n+    supports_vector_queries: bool = True\n+\n+    def __init__(\n+        self,\n+        db_connection_string: str,\n+        embedding_dim: int = 1536,\n+        node_table_name: str = \"pg_nodes\",\n+        relation_table_name: str = \"pg_relations\",\n+        drop_existing_table: bool = False,\n+        echo_queries: bool = False,\n+    ) -> None:\n+        # TiDB Serverless clusters have a limitation: if there are no active connections for 5 minutes,\n+        # they will shut down, which closes all connections, so we need to recycle the connections\n+        self._engine = create_engine(\n+            db_connection_string, pool_recycle=300, echo=echo_queries\n+        )\n+        check_db_availability(self._engine, check_vector=True)\n+\n+        self._embedding_dim = embedding_dim\n+        self._node_table_name = node_table_name\n+        self._relation_table_name = relation_table_name\n+        self._drop_existing_table = drop_existing_table\n+        self._node_model, self._relation_model = self.init_schema()\n+\n+    def init_schema(self) -> Tuple:\n+        \"\"\"Initialize schema.\"\"\"\n+        Base = declarative_base()\n+\n+        class BaseMixin:\n+            created_at = Column(DateTime, nullable=False, server_default=sql.func.now())\n+            updated_at = Column(\n+                DateTime,\n+                nullable=False,\n+                server_default=sql.func.now(),\n+                onupdate=sql.func.now(),\n+            )\n+\n+        class NodeModel(BaseMixin, Base):\n+            __tablename__ = self._node_table_name\n+            id = Column(String(512), primary_key=True)\n+            text = Column(TEXT, nullable=True)\n+            name = Column(String(512), nullable=True)\n+            label = Column(String(512), nullable=False, default=\"node\")\n+            properties = Column(JSON, default={})\n+            embedding = Column(\n+                VectorType(self._embedding_dim), comment=\"hnsw(distance=cosine)\"\n+            )\n+\n+        class RelationModel(BaseMixin, Base):\n+            __tablename__ = self._relation_table_name\n+            id = Column(Integer, primary_key=True)\n+            label = Column(String(512), nullable=False)\n+            source_id = Column(String(512), ForeignKey(f\"{self._node_table_name}.id\"))\n+            target_id = Column(String(512), ForeignKey(f\"{self._node_table_name}.id\"))\n+            properties = Column(JSON, default={})\n+\n+            source = relationship(\"NodeModel\", foreign_keys=[source_id])\n+            target = relationship(\"NodeModel\", foreign_keys=[target_id])\n+\n+        if self._drop_existing_table:\n+            Base.metadata.drop_all(self._engine)\n+        Base.metadata.create_all(self._engine)\n+        return NodeModel, RelationModel\n+\n+    def get(\n+        self,\n+        properties: Optional[dict] = None,\n+        ids: Optional[List[str]] = None,\n+    ) -> List[LabelledNode]:\n+        \"\"\"Get nodes.\"\"\"\n+        with Session(self._engine) as session:\n+            query = session.query(self._node_model)\n+            if properties:\n+                for key, value in properties.items():\n+                    query = query.filter(self._node_model.properties[key] == value)\n+            if ids:\n+                query = query.filter(self._node_model.id.in_(ids))\n+\n+            nodes = []\n+            for n in query.all():\n+                if n.text and n.name is None:\n+                    nodes.append(\n+                        ChunkNode(\n+                            id=n.id,\n+                            text=n.text,\n+                            label=n.label,\n+                            properties=remove_empty_values(n.properties),\n+                        )\n+                    )\n+                else:\n+                    nodes.append(\n+                        EntityNode(\n+                            name=n.name,\n+                            label=n.label,\n+                            properties=remove_empty_values(n.properties),\n+                        )\n+                    )\n+            return nodes\n+\n+    def get_triplets(\n+        self,\n+        entity_names: Optional[List[str]] = None,\n+        relation_names: Optional[List[str]] = None,\n+        properties: Optional[dict] = None,\n+        ids: Optional[List[str]] = None,\n+    ) -> List[Triplet]:\n+        \"\"\"Get triplets.\"\"\"\n+        # if nothing is passed, return empty list\n+        if not ids and not properties and not entity_names and not relation_names:\n+            return []\n+\n+        with Session(self._engine) as session:\n+            query = session.query(self._relation_model).options(\n+                joinedload(self._relation_model.source),\n+                joinedload(self._relation_model.target),\n+            )\n+            if ids:\n+                query = query.filter(\n+                    self._relation_model.source_id.in_(ids)\n+                    | self._relation_model.target_id.in_(ids)\n+                )\n+            if properties:\n+                for key, value in properties.items():\n+                    query = query.filter(\n+                        (self._relation_model.properties[key] == value)\n+                        | self._relation_model.source.has(\n+                            self._node_model.properties[key] == value\n+                        )\n+                        | self._relation_model.target.has(\n+                            self._node_model.properties[key] == value\n+                        )\n+                    )\n+            if entity_names:\n+                query = query.filter(\n+                    self._relation_model.source.has(\n+                        self._node_model.name.in_(entity_names)\n+                    )\n+                    | self._relation_model.target.has(\n+                        self._node_model.name.in_(entity_names)\n+                    )\n+                )\n+            if relation_names:\n+                query = query.filter(self._relation_model.label.in_(relation_names))\n+\n+            triplets = []\n+            for r in query.all():\n+                source = EntityNode(\n+                    name=r.source.name,\n+                    label=r.source.label,\n+                    properties=remove_empty_values(r.source.properties),\n+                )\n+                target = EntityNode(\n+                    name=r.target.name,\n+                    label=r.target.label,\n+                    properties=remove_empty_values(r.target.properties),\n+                )\n+                relation = Relation(\n+                    label=r.label,\n+                    source_id=source.id,\n+                    target_id=target.id,\n+                    properties=remove_empty_values(r.properties),\n+                )\n+                triplets.append([source, relation, target])\n+            return triplets\n+\n+    def get_rel_map(\n+        self,\n+        graph_nodes: List[LabelledNode],\n+        depth: int = 2,\n+        limit: int = 30,\n+        ignore_rels: Optional[List[str]] = None,\n+    ) -> List[Triplet]:\n+        \"\"\"Get depth-aware rel map.\"\"\"\n+        triplets = []\n+        ids = [node.id for node in graph_nodes]\n+\n+        if not ids:\n+            return []\n+\n+        with Session(self._engine) as session:\n+            result = session.execute(\n+                sql.text(\n+                    rel_depth_query.format(\n+                        relation_table=self._relation_table_name,\n+                        node_table=self._node_table_name,\n+                    )\n+                ),\n+                {\n+                    \"ids\": ids,\n+                    \"depth\": depth,\n+                    \"limit\": limit,\n+                },\n+            )\n+\n+            keys = result.keys()\n+            raw_rels = [dict(zip(keys, row)) for row in result.fetchall()]\n+\n+            ignore_rels = ignore_rels or []\n+            for row in raw_rels:\n+                if row[\"rel_label\"] in ignore_rels:\n+                    continue\n+\n+                source = EntityNode(\n+                    id=row[\"e1_id\"],\n+                    name=row[\"e1_name\"],\n+                    label=row[\"e1_label\"],\n+                    properties=json.loads(row[\"e1_properties\"]),\n+                )\n+                target = EntityNode(\n+                    id=row[\"e2_id\"],\n+                    name=row[\"e2_name\"],\n+                    label=row[\"e2_label\"],\n+                    properties=json.loads(row[\"e2_properties\"]),\n+                )\n+                relation = Relation(\n+                    label=row[\"rel_label\"],\n+                    source_id=source.id,\n+                    target_id=target.id,\n+                    properties=json.loads(row[\"rel_properties\"]),\n+                )\n+                triplets.append([source, relation, target])\n+        return triplets\n+\n+    def upsert_nodes(self, nodes: List[LabelledNode]) -> None:\n+        \"\"\"Upsert nodes.\"\"\"\n+        entity_list: List[EntityNode] = []\n+        chunk_list: List[ChunkNode] = []\n+        other_list: List[LabelledNode] = []\n+\n+        for item in nodes:\n+            if isinstance(item, EntityNode):\n+                entity_list.append(item)\n+            elif isinstance(item, ChunkNode):\n+                chunk_list.append(item)\n+            else:\n+                other_list.append(item)\n+\n+        with Session(self._engine) as session:\n+            # TODO: use upsert instead of get_or_create\n+            for entity in entity_list:\n+                entity_instance, _ = get_or_create(\n+                    session, self._node_model, id=entity.id\n+                )\n+                entity_instance.name = entity.name\n+                entity_instance.label = entity.label\n+                entity_instance.properties = entity.properties\n+                entity_instance.embedding = entity.embedding\n+                session.add(entity_instance)\n+\n+            for chunk in chunk_list:\n+                chunk_instance, _ = get_or_create(\n+                    session, self._node_model, id=chunk.id\n+                )\n+                chunk_instance.text = chunk.text\n+                chunk_instance.label = chunk.label\n+                chunk_instance.properties = chunk.properties\n+                chunk_instance.embedding = chunk.embedding\n+                session.add(chunk_instance)\n+            session.commit()\n+\n+    def upsert_relations(self, relations: List[Relation]) -> None:\n+        \"\"\"Upsert relations.\"\"\"\n+        with Session(self._engine) as session:\n+            for r in relations:\n+                get_or_create(\n+                    session,\n+                    self._node_model,\n+                    id=r.source_id,\n+                )\n+                get_or_create(\n+                    session,\n+                    self._node_model,\n+                    id=r.target_id,\n+                )\n+                relation_instance, _ = get_or_create(\n+                    session,\n+                    self._relation_model,\n+                    label=r.label,\n+                    source_id=r.source_id,\n+                    target_id=r.target_id,\n+                )\n+                relation_instance.properties = r.properties\n+                session.add(relation_instance)\n+                session.commit()\n+\n+    def delete(\n+        self,\n+        entity_names: Optional[List[str]] = None,\n+        relation_names: Optional[List[str]] = None,\n+        properties: Optional[dict] = None,\n+        ids: Optional[List[str]] = None,\n+    ) -> None:\n+        \"\"\"Delete matching data.\"\"\"\n+        with Session(self._engine) as session:\n+            # 1. Delete relations\n+            relation_stmt = delete(self._relation_model)\n+            if ids:\n+                relation_stmt = relation_stmt.filter(\n+                    self._relation_model.source_id.in_(ids)\n+                    | self._relation_model.target_id.in_(ids)\n+                )\n+            if entity_names:\n+                relation_stmt = relation_stmt.filter(\n+                    self._relation_model.source.has(name=entity_names)\n+                    | self._relation_model.target.has(name=entity_names)\n+                )\n+            if relation_names:\n+                relation_stmt = relation_stmt.filter(\n+                    self._relation_model.label.in_(relation_names)\n+                )\n+            if properties:\n+                for key, value in properties.items():\n+                    relation_stmt = relation_stmt.filter(\n+                        self._relation_model.source.has(\n+                            self._node_model.properties[key] == value\n+                        )\n+                        | self._relation_model.target.has(\n+                            self._node_model.properties[key] == value\n+                        )\n+                    )\n+            session.execute(relation_stmt)\n+\n+            # 2. Delete nodes\n+            entity_stmt = delete(self._node_model)\n+            if ids:\n+                entity_stmt = entity_stmt.filter(self._node_model.id.in_(ids))\n+            if entity_names:\n+                entity_stmt = entity_stmt.filter(\n+                    self._node_model.name.in_(entity_names)\n+                )\n+            if properties:\n+                for key, value in properties.items():\n+                    entity_stmt = entity_stmt.filter(\n+                        self._node_model.properties[key] == value\n+                    )\n+            session.execute(entity_stmt)\n+            session.commit()\n+\n+    def structured_query(\n+        self, query: str, param_map: Optional[Dict[str, Any]] = None\n+    ) -> Any:\n+        \"\"\"Query the graph store with statement and parameters.\"\"\"\n+        raise NotImplementedError(\"TiDB does not support cypher queries.\")\n+\n+    def vector_query(\n+        self, query: VectorStoreQuery, **kwargs: Any\n+    ) -> Tuple[List[LabelledNode], List[float]]:\n+        \"\"\"Query the graph store with a vector store query.\"\"\"\n+        with Session(self._engine) as session:\n+            result = (\n+                session.query(\n+                    self._node_model,\n+                    self._node_model.embedding.cosine_distance(\n+                        query.query_embedding\n+                    ).label(\"embedding_distance\"),\n+                )\n+                .filter(self._node_model.name.is_not(None))\n+                .order_by(sql.asc(\"embedding_distance\"))\n+                .limit(query.similarity_top_k)\n+                .all()\n+            )\n+\n+            nodes = []\n+            scores = []\n+            for node, score in result:\n+                nodes.append(\n+                    EntityNode(\n+                        name=node.name,\n+                        label=node.label,\n+                        properties=remove_empty_values(node.properties),\n+                    )\n+                )\n+                scores.append(score)\n+            return nodes, scores",
      "status": "added",
      "changes": 451,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Fproperty_graph.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Fproperty_graph.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/property_graph.py",
      "additions": 451,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Fproperty_graph.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "064c1386a088575c9abba6a3d7e04a23ac99fc6b",
      "patch": "@@ -0,0 +1,52 @@\n+from sqlalchemy.orm import Session\n+from sqlalchemy import Engine, exc, sql\n+\n+\n+def check_db_availability(engine: Engine, check_vector: bool = False) -> None:\n+    try:\n+        with engine.connect() as conn:\n+            if check_vector:\n+                conn.execute(sql.text(\"\"\"SELECT Vec_Dims(\"[1]\");\"\"\"))\n+            else:\n+                conn.execute(sql.text(\"\"\"SELECT 1;\"\"\"))\n+    except exc.DatabaseError as e:\n+        db_error_code = e.orig.args[0]\n+        if db_error_code == 1045:\n+            raise ValueError(\n+                \"Could not connect to the TiDB server. \"\n+                \"Please check if the connection string is correct.\"\n+            ) from e\n+        elif db_error_code == 1305:\n+            raise ValueError(\n+                \"Please confirm if your TiDB supports vector search. \"\n+                \"You can check this by running the query `SELECT Vec_Dims('[1]')` in TiDB.\"\n+            ) from e\n+        else:\n+            raise ValueError(\n+                \"An error occurred while checking the database availability.\"\n+            ) from e\n+\n+\n+def get_or_create(session: Session, model, **kwargs):\n+    instance = session.query(model).filter_by(**kwargs).first()\n+    if instance:\n+        return instance, False\n+    else:\n+        instance = model(**kwargs)\n+        session.add(instance)\n+        session.commit()\n+        return instance, True\n+\n+\n+def remove_empty_values(input_dict):\n+    \"\"\"\n+    Remove entries with empty values from the dictionary.\n+\n+    Parameters:\n+    input_dict (dict): The dictionary from which empty values need to be removed.\n+\n+    Returns:\n+    dict: A new dictionary with all empty values removed.\n+    \"\"\"\n+    # Create a new dictionary excluding empty values\n+    return {key: value for key, value in input_dict.items() if value}",
      "status": "added",
      "changes": 52,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Futils.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Futils.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/llama_index/graph_stores/tidb/utils.py",
      "additions": 52,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fllama_index%2Fgraph_stores%2Ftidb%2Futils.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "9745b178b841461740276a6c986d3308fc8f8ea5",
      "patch": "@@ -0,0 +1,55 @@\n+[build-system]\n+build-backend = \"poetry.core.masonry.api\"\n+requires = [\"poetry-core\"]\n+\n+[tool.codespell]\n+check-filenames = true\n+check-hidden = true\n+skip = \"*.csv,*.html,*.json,*.jsonl,*.pdf,*.txt,*.ipynb\"\n+\n+[tool.llamahub]\n+contains_example = false\n+import_path = \"llama_index.graph_stores.tidb\"\n+\n+[tool.llamahub.class_authors]\n+TiDBGraphStore = \"llama-index\"\n+TiDBPropertyGraphStore = \"llama-index\"\n+\n+[tool.mypy]\n+disallow_untyped_defs = true\n+exclude = [\"_static\", \"build\", \"examples\", \"notebooks\", \"venv\"]\n+ignore_missing_imports = true\n+python_version = \"3.8\"\n+\n+[tool.poetry]\n+authors = [\"Your Name <you@example.com>\"]\n+description = \"llama-index graph stores tidb integration\"\n+exclude = [\"**/BUILD\"]\n+license = \"MIT\"\n+name = \"llama-index-graph-stores-tidb\"\n+readme = \"README.md\"\n+version = \"0.1.0\"\n+\n+[tool.poetry.dependencies]\n+python = \">=3.8.1,<4.0\"\n+tidb-vector = \"^0.0.9\"\n+PyMySQL = \"^1.1.1\"\n+SQLAlchemy = \"^2.0.30\"\n+llama-index-core = \"^0.10.40\"\n+\n+[tool.poetry.group.dev.dependencies]\n+jupyter = \"^1.0.0\"\n+mypy = \"^1.10.0\"\n+pre-commit = \"3.2.0\"\n+pytest = \"7.2.1\"\n+\n+[tool.poetry.group.dev.dependencies.black]\n+extras = [\"jupyter\"]\n+version = \"<=23.9.1,>=23.7.0\"\n+\n+[tool.poetry.group.dev.dependencies.codespell]\n+extras = [\"toml\"]\n+version = \">=v2.2.6\"\n+\n+[[tool.poetry.packages]]\n+include = \"llama_index/\"",
      "status": "added",
      "changes": 55,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fpyproject.toml",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fpyproject.toml",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/pyproject.toml",
      "additions": 55,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Fpyproject.toml?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "dabf212d7e7162849c24a733909ac4f645d75a31",
      "patch": "@@ -0,0 +1 @@\n+python_tests()",
      "status": "added",
      "changes": 1,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2FBUILD",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2FBUILD",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/tests/BUILD",
      "additions": 1,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2FBUILD?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
      "status": "added",
      "changes": 0,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2F__init__.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2F__init__.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/tests/__init__.py",
      "additions": 0,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2F__init__.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "8e4e6b24598688a66cf808f8be403e151eee6b3a",
      "patch": "@@ -0,0 +1,10 @@\n+from unittest.mock import MagicMock, patch\n+\n+from llama_index.core.graph_stores.types import GraphStore\n+from llama_index.graph_stores.tidb import TiDBGraphStore\n+\n+\n+@patch(\"llama_index.graph_stores.tidb.TiDBGraphStore\")\n+def test_tidb_graph_store(MockTiDBGraphStore: MagicMock):\n+    instance: TiDBGraphStore = MockTiDBGraphStore.return_value()\n+    assert isinstance(instance, GraphStore)",
      "status": "added",
      "changes": 10,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2Ftest_graph_stores_tidb.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2Ftest_graph_stores_tidb.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/tests/test_graph_stores_tidb.py",
      "additions": 10,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2Ftest_graph_stores_tidb.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    },
    {
      "sha": "931521d977dde9bbbe140df30c22747f018ae805",
      "patch": "@@ -0,0 +1,69 @@\n+import os\n+from unittest import TestCase, SkipTest\n+\n+from llama_index.core.graph_stores.types import (\n+    EntityNode,\n+    Relation,\n+)\n+\n+from llama_index.graph_stores.tidb import TiDBPropertyGraphStore\n+\n+\n+def get_store():\n+    return TiDBPropertyGraphStore(\n+        db_connection_string=os.environ.get(\"TIDB_TEST_CONNECTION_STRING\"),\n+        drop_existing_table=True,\n+        relation_table_name=\"test_relations\",\n+        node_table_name=\"test_nodes\",\n+    )\n+\n+\n+class TestTiDBPropertyGraphStore(TestCase):\n+    @classmethod\n+    def setUp(self) -> None:\n+        try:\n+            get_store()\n+        except Exception:\n+            raise SkipTest(\"TiDB cluster is not available\")\n+\n+        self.e1 = EntityNode(name=\"e1\", properties={\"p1\": \"v1\"})\n+        self.e2 = EntityNode(name=\"e2\")\n+        self.r = Relation(label=\"r\", source_id=self.e1.id, target_id=self.e2.id)\n+\n+    def test_add(self):\n+        g = get_store()\n+\n+        g.upsert_nodes([self.e1, self.e2])\n+        g.upsert_relations([self.r])\n+        assert len(g.get_triplets(entity_names=[\"e1\"])) == 1\n+        assert len(g.get_triplets(entity_names=[\"e3\"])) == 0\n+        assert len(g.get_triplets(properties={\"p1\": \"v1\"})) == 1\n+        assert len(g.get_triplets(properties={\"p1\": \"v2\"})) == 0\n+\n+    def test_delete_by_entity_names(self):\n+        g = get_store()\n+\n+        g.upsert_nodes([self.e1, self.e2])\n+        g.upsert_relations([self.r])\n+        assert len(g.get_triplets(entity_names=[\"e1\"])) == 1\n+        g.delete(entity_names=[\"e1\"])\n+        assert len(g.get_triplets(entity_names=[\"e1\"])) == 0\n+\n+    def test_delete_by_entity_properties(self):\n+        g = get_store()\n+\n+        g.upsert_nodes([self.e1, self.e2])\n+        g.upsert_relations([self.r])\n+        assert len(g.get_triplets(entity_names=[\"e1\"])) == 1\n+        g.delete(properties={\"p1\": \"not exist\"})\n+        assert len(g.get_triplets(entity_names=[\"e1\"])) == 1\n+        g.delete(properties={\"p1\": \"v1\"})\n+        assert len(g.get_triplets(entity_names=[\"e1\"])) == 0\n+\n+    def test_get(self):\n+        g = get_store()\n+\n+        g.upsert_nodes([self.e1, self.e2])\n+        assert len(g.get(ids=[self.e1.id])) == 1\n+        assert len(g.get(ids=[self.e1.id, self.e2.id])) == 2\n+        assert len(g.get(properties={\"p1\": \"v1\"})) == 1",
      "status": "added",
      "changes": 69,
      "raw_url": "https://github.com/run-llama/llama_index/raw/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2Ftest_property_graph_stores_tidb.py",
      "blob_url": "https://github.com/run-llama/llama_index/blob/92011f61d3aadfb306b8d2a8c872a97d895be21f/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2Ftest_property_graph_stores_tidb.py",
      "filename": "llama-index-integrations/graph_stores/llama-index-graph-stores-tidb/tests/test_property_graph_stores_tidb.py",
      "additions": 69,
      "deletions": 0,
      "contents_url": "https://api.github.com/repos/run-llama/llama_index/contents/llama-index-integrations%2Fgraph_stores%2Fllama-index-graph-stores-tidb%2Ftests%2Ftest_property_graph_stores_tidb.py?ref=92011f61d3aadfb306b8d2a8c872a97d895be21f"
    }
  ],
  "stats": {
    "total": 1731,
    "additions": 1731,
    "deletions": 0
  },
  "author": {
    "id": 10102304,
    "url": "https://api.github.com/users/wd0517",
    "type": "User",
    "login": "wd0517",
    "node_id": "MDQ6VXNlcjEwMTAyMzA0",
    "html_url": "https://github.com/wd0517",
    "gists_url": "https://api.github.com/users/wd0517/gists{/gist_id}",
    "repos_url": "https://api.github.com/users/wd0517/repos",
    "avatar_url": "https://avatars.githubusercontent.com/u/10102304?v=4",
    "events_url": "https://api.github.com/users/wd0517/events{/privacy}",
    "site_admin": false,
    "gravatar_id": "",
    "starred_url": "https://api.github.com/users/wd0517/starred{/owner}{/repo}",
    "followers_url": "https://api.github.com/users/wd0517/followers",
    "following_url": "https://api.github.com/users/wd0517/following{/other_user}",
    "organizations_url": "https://api.github.com/users/wd0517/orgs",
    "subscriptions_url": "https://api.github.com/users/wd0517/subscriptions",
    "received_events_url": "https://api.github.com/users/wd0517/received_events"
  },
  "commit": {
    "url": "https://api.github.com/repos/run-llama/llama_index/git/commits/92011f61d3aadfb306b8d2a8c872a97d895be21f",
    "tree": {
      "sha": "07b0089379ce160c6be1e30864717ebfb4955c36",
      "url": "https://api.github.com/repos/run-llama/llama_index/git/trees/07b0089379ce160c6be1e30864717ebfb4955c36"
    },
    "author": {
      "date": "2024-06-24T15:39:05Z",
      "name": "WD",
      "email": "me@wangdi.ink"
    },
    "message": "Add tidb knowledge graph store (#14259)",
    "committer": {
      "date": "2024-06-24T15:39:05Z",
      "name": "GitHub",
      "email": "noreply@github.com"
    },
    "verification": {
      "reason": "valid",
      "payload": "tree 07b0089379ce160c6be1e30864717ebfb4955c36\nparent 4ba35d98ccda19358f75d9a23730f3ac43a0cfb2\nauthor WD <me@wangdi.ink> 1719243545 +0800\ncommitter GitHub <noreply@github.com> 1719243545 +0000\n\nAdd tidb knowledge graph store (#14259)\n\n",
      "verified": true,
      "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsFcBAABCAAQBQJmeZMZCRC1aQ7uu5UhlAAALToQAB6nO1Uqlp4NUzyG6vEk/mR5\nkern+460orAiwwLkz3QMht97izJGjSLxiHs/Gam5qXdeAjaof2p1FY24T8h0O5Bs\nrv7yffEFJvFJGHs1oepSPkVycvQCQqxefDAi2wugeJiMWmt0G+sv2kSEKoR+M+Rm\nhhiUETjEVvdWvop/QexaL39KBRp5apl3jNax+iyNh53W3YEcq2psDvpeyQgRtr+7\nLeuzfkpvsbPILek0381HAZobzB1tObbzMizizf9A7mk6ahbUnZxbYmOJvtS5z7FN\n0QYWfmjj4duZKdRz5n2apj3burlyEulhvD3tD3hUHqfpFgso5zfD4afqT8pSqo+d\nrObYsdeOeUpvBv/0w6oMNt5dbnpfHcvh8KlS0qniowj8pxTe3UsqPgHzA1a0y8m+\nhz0PMJ7nRQQ5B+2ngW1zxMGtpIg84SojZ09IPTAzW1ot1mNwyS+NgJGx9fT8iSRs\npvlooOBhrztU0JnFkWbVwS2ZBjyO4rUIjh1QQ0VNyUmJL5UlV35WZl+YOGnNUIpT\nV7duCZ2yFtHpcMtS8TKUfzYntG/EQVaMF39XOhODc4txNAT8YKFcJDeCmgLUl7Ts\n2E57Un48veIPRIFL1tan72Ei2ds3dYd0YMFbyhjMB3j8iHuggB7PqAIgE1yXuVpA\n2ceM2MfoHaqQzzeERsfQ\n=tAnc\n-----END PGP SIGNATURE-----\n"
    },
    "comment_count": 0
  },
  "node_id": "C_kwDOIWuq59oAKDkyMDExZjYxZDNhYWRmYjMwNmI4ZDJhOGM4NzJhOTdkODk1YmUyMWY",
  "parents": [
    {
      "sha": "4ba35d98ccda19358f75d9a23730f3ac43a0cfb2",
      "url": "https://api.github.com/repos/run-llama/llama_index/commits/4ba35d98ccda19358f75d9a23730f3ac43a0cfb2",
      "html_url": "https://github.com/run-llama/llama_index/commit/4ba35d98ccda19358f75d9a23730f3ac43a0cfb2"
    }
  ],
  "html_url": "https://github.com/run-llama/llama_index/commit/92011f61d3aadfb306b8d2a8c872a97d895be21f",
  "committer": {
    "id": 19864447,
    "url": "https://api.github.com/users/web-flow",
    "type": "User",
    "login": "web-flow",
    "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
    "html_url": "https://github.com/web-flow",
    "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
    "repos_url": "https://api.github.com/users/web-flow/repos",
    "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4",
    "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
    "site_admin": false,
    "gravatar_id": "",
    "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
    "followers_url": "https://api.github.com/users/web-flow/followers",
    "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
    "organizations_url": "https://api.github.com/users/web-flow/orgs",
    "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
    "received_events_url": "https://api.github.com/users/web-flow/received_events"
  },
  "comments_url": "https://api.github.com/repos/run-llama/llama_index/commits/92011f61d3aadfb306b8d2a8c872a97d895be21f/comments"
}